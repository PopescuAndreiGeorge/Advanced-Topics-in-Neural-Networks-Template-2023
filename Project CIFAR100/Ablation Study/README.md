## Ablation Study

* The main goal was to study the architecture implemented both through the use of different models (ResNet-18, ResNet-34, PreResNet) but also to modify the content and the number of coarse classes that end up using the models. I worked with their hyperparameters to find a combination that gives good results for each class of elements. Also, I studied the behavior of the architecture when the number of coarse classes and the elements that populate them is changed, with the aim of reaching a conclusion related to the efficiency of the use of several classes and the way in which they should be composed, in relation to the results obtained and the time required for training, taking into account the required memory space.
* From the tests carried out, the aspects that most influence the results obtained are the types of networks used for each individual class, but also the structure of the architecture. By dividing the data into only 3 rough classes, I noticed that it gives better results in certain cases than if 4 classes are used, here the way in which the data was distributed to the classes is equally important, moving the elements from one class to another based on similarities influence the speed of learning. 
* Also, the use of PreResNets for models that use data from coarse classes has proven to be more efficient in terms of accuracy, although the training time is longer than in the case of other models. 
* For the case of training on the CPU, we were able to use the model with 4 coarse classes, each of them using a ResNet-18 type model to obtain a shorter training time. Only 85 epochs were used to train this architecture, obtaining top 1% accuracy of 42.09%, top 3 accuracy of 66.33%. This training lasted one hour and 30 minutes.

